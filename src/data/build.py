"""
–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö, Feature Engineering –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
"""

import json
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, LabelEncoder

from .common import get_project_root, load_config, ensure_dir
from .ingest import load_bronze_data


def create_feature_schema(
    df: pd.DataFrame,
    config: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    –°–æ–∑–¥–∞—Ç—å —Å—Ö–µ–º—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (feature contract)

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ö–µ–º–æ–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    """
    if config is None:
        config = load_config()

    drop_cols = config["ingestion"]["drop_columns"]
    target_col = config["ingestion"]["target_column"]

    # –í—Å–µ –∫–æ–ª–æ–Ω–∫–∏
    all_cols = df.columns.tolist()

    # –°–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞–º–∏)
    meta_cols = [c for c in all_cols if c.startswith('_')]

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—É–±–ª–∏—Ä—É—é—â–µ–≥–æ—Å—è —Å—Ç–æ–ª–±—Ü–∞ Fwd Header Length
    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º .1 –≤–µ—Ä—Å–∏—é
    duplicate_cols = [c for c in all_cols if '.1' in c]

    # –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
    cols_to_drop = drop_cols + meta_cols + [target_col] + duplicate_cols

    # –ü—Ä–∏–∑–Ω–∞–∫–∏
    feature_cols = [c for c in all_cols if c not in cols_to_drop]

    # –¢–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    numeric_features = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()
    categorical_features = df[feature_cols].select_dtypes(exclude=[np.number]).columns.tolist()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –Ω–∞–∑–≤–∞–Ω–∏—è
    seen = set()
    unique_numeric = []
    for col in numeric_features:
        if col not in seen:
            seen.add(col)
            unique_numeric.append(col)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
    feature_stats = {}
    for col in unique_numeric:
        valid_data = df[col].replace([np.inf, -np.inf], np.nan).dropna()
        if len(valid_data) > 0:
            feature_stats[col] = {
                "dtype": str(df[col].dtype),
                "min": float(valid_data.min()),
                "max": float(valid_data.max()),
                "mean": float(valid_data.mean()),
                "std": float(valid_data.std()) if valid_data.std() == valid_data.std() else 0.0,
                "median": float(valid_data.median()),
                "q01": float(valid_data.quantile(0.01)),
                "q99": float(valid_data.quantile(0.99)),
                "null_count": int(df[col].isna().sum()),
                "inf_count": int(((df[col] == np.inf) | (df[col] == -np.inf)).sum())
            }

    schema = {
        "version": "1.0",
        "target_column": target_col,
        "feature_columns": unique_numeric,
        "categorical_columns": categorical_features,
        "drop_columns": cols_to_drop,
        "duplicate_columns": duplicate_cols,
        "meta_columns": meta_cols,
        "total_features": len(unique_numeric),
        "feature_stats": feature_stats
    }

    return schema


def clean_data(
    df: Optional[pd.DataFrame] = None,
    config: Optional[Dict[str, Any]] = None,
    schema: Optional[Dict[str, Any]] = None
) -> pd.DataFrame:
    """
    –û—á–∏—Å—Ç–∏—Ç—å –¥–∞–Ω–Ω—ã–µ

    –®–∞–≥–∏:
    1. –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã—Ö/–±–∏—Ç—ã—Ö —Å—Ç—Ä–æ–∫
    2. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    3. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∫–æ–ª–æ–Ω–æ–∫
    4. –û–±—Ä–∞–±–æ—Ç–∫–∞ Inf
    5. –û–±—Ä–∞–±–æ—Ç–∫–∞ NaN
    6. –ö–ª–∏–ø–ø–∏–Ω–≥ –≤—ã–±—Ä–æ—Å–æ–≤
    """
    if config is None:
        config = load_config()

    if df is None:
        df = load_bronze_data(config)

    # –í–ê–ñ–ù–û: —Å–æ–∑–¥–∞—ë–º –∫–æ–ø–∏—é —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å SettingWithCopyWarning
    df = df.copy()

    cleaning_config = config["cleaning"]
    target_col = config["ingestion"]["target_column"]

    print("="*60)
    print("–û–ß–ò–°–¢–ö–ê –î–ê–ù–ù–´–•")
    print("="*60)

    initial_rows = len(df)
    print(f"\nüìä –ò—Å—Ö–æ–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {initial_rows:,}")

    # 0. –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ (–≥–¥–µ –¥–∞–∂–µ Label –ø—É—Å—Ç–æ–π)
    empty_mask = df[target_col].isna()
    if empty_mask.sum() > 0:
        print(f"\nüìä –£–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ (Label is NaN)...")
        print(f"   –ù–∞–π–¥–µ–Ω–æ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫: {empty_mask.sum():,}")
        df = df[~empty_mask].copy()
        print(f"   –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è: {len(df):,}")

    # 1. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    if cleaning_config["remove_duplicates"]:
        before_dup = len(df)
        # –£–±–∏—Ä–∞–µ–º –º–µ—Ç–∞-–∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        check_cols = [c for c in df.columns if not c.startswith('_')]
        df = df.drop_duplicates(subset=check_cols, keep='first').copy()
        removed = before_dup - len(df)
        print(f"\nüìä –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
        print(f"   –£–¥–∞–ª–µ–Ω–æ: {removed:,}")
        print(f"   –û—Å—Ç–∞–ª–æ—Å—å: {len(df):,}")

    # 2. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∫–æ–ª–æ–Ω–æ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, Fwd Header Length.1)
    dup_cols = [c for c in df.columns if '.1' in c]
    if dup_cols:
        print(f"\nüìä –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∫–æ–ª–æ–Ω–æ–∫: {dup_cols}")
        df = df.drop(columns=dup_cols)

    # –ü–æ–ª—É—á–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols = [c for c in numeric_cols if not c.startswith('_')]

    # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ Inf
    inf_replacement = cleaning_config["inf_replacement"]
    print(f"\nüìä –û–±—Ä–∞–±–æ—Ç–∫–∞ Inf (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {inf_replacement})...")

    total_inf_replaced = 0
    for col in numeric_cols:
        inf_mask = (df[col] == np.inf) | (df[col] == -np.inf)
        inf_count = inf_mask.sum()

        if inf_count > 0:
            total_inf_replaced += inf_count

            if inf_replacement == "nan":
                df.loc[inf_mask, col] = np.nan
            elif inf_replacement == "clip":
                valid_data = df.loc[~inf_mask, col]
                if len(valid_data) > 0:
                    max_val = valid_data.max()
                    min_val = valid_data.min()
                    df.loc[df[col] == np.inf, col] = max_val
                    df.loc[df[col] == -np.inf, col] = min_val
            elif inf_replacement == "median":
                valid_data = df.loc[~inf_mask, col]
                if len(valid_data) > 0:
                    median_val = valid_data.median()
                    df.loc[inf_mask, col] = median_val

    print(f"   –ó–∞–º–µ–Ω–µ–Ω–æ Inf –∑–Ω–∞—á–µ–Ω–∏–π: {total_inf_replaced:,}")

    # 4. –û–±—Ä–∞–±–æ—Ç–∫–∞ NaN
    nan_strategy = cleaning_config["nan_strategy"]
    print(f"\nüìä –û–±—Ä–∞–±–æ—Ç–∫–∞ NaN (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {nan_strategy})...")

    nan_before = df[numeric_cols].isna().sum().sum()

    if nan_strategy == "median":
        for col in numeric_cols:
            nan_count = df[col].isna().sum()
            if nan_count > 0:
                median_val = df[col].median()
                # –ï—Å–ª–∏ –º–µ–¥–∏–∞–Ω–∞ —Ç–æ–∂–µ NaN, –∏—Å–ø–æ–ª—å–∑—É–µ–º 0
                if pd.isna(median_val):
                    median_val = 0
                df.loc[df[col].isna(), col] = median_val
    elif nan_strategy == "mean":
        for col in numeric_cols:
            nan_count = df[col].isna().sum()
            if nan_count > 0:
                mean_val = df[col].mean()
                if pd.isna(mean_val):
                    mean_val = 0
                df.loc[df[col].isna(), col] = mean_val
    elif nan_strategy == "zero":
        for col in numeric_cols:
            df.loc[df[col].isna(), col] = 0
    elif nan_strategy == "drop":
        df = df.dropna(subset=numeric_cols).copy()

    nan_after = df[numeric_cols].isna().sum().sum()
    print(f"   NaN –¥–æ: {nan_before:,}, –ø–æ—Å–ª–µ: {nan_after:,}")

    # 5. –ö–ª–∏–ø–ø–∏–Ω–≥ –≤—ã–±—Ä–æ—Å–æ–≤
    if cleaning_config["clip_outliers"]:
        lower_pct = cleaning_config["clip_lower_percentile"]
        upper_pct = cleaning_config["clip_upper_percentile"]
        print(f"\nüìä –ö–ª–∏–ø–ø–∏–Ω–≥ –≤—ã–±—Ä–æ—Å–æ–≤ ({lower_pct}-{upper_pct} percentile)...")

        clipped_cols = 0
        for col in numeric_cols:
            lower = df[col].quantile(lower_pct)
            upper = df[col].quantile(upper_pct)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≥—Ä–∞–Ω–∏—Ü—ã –≤–∞–ª–∏–¥–Ω—ã
            if pd.notna(lower) and pd.notna(upper) and lower < upper:
                before_clip = ((df[col] < lower) | (df[col] > upper)).sum()
                if before_clip > 0:
                    df.loc[:, col] = df[col].clip(lower, upper)
                    clipped_cols += 1

        print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–æ–ª–æ–Ω–æ–∫: {clipped_cols}")

    print(f"\n‚úÖ –ò—Ç–æ–≥–æ —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(df):,}")
    print(f"   –£–¥–∞–ª–µ–Ω–æ –≤—Å–µ–≥–æ: {initial_rows - len(df):,} ({100*(initial_rows - len(df))/initial_rows:.1f}%)")

    return df


def preprocess_data(
    df: pd.DataFrame,
    config: Optional[Dict[str, Any]] = None,
    schema: Optional[Dict[str, Any]] = None,
    fit: bool = True,
    preprocessor: Optional[Any] = None
) -> Tuple[pd.DataFrame, Dict[str, Any], Any]:
    """
    –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö: —Å–∫–µ–π–ª–∏–Ω–≥ –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ

    Args:
        df: DataFrame
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        schema: –°—Ö–µ–º–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        fit: –û–±—É—á–∞—Ç—å –ª–∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä (True –¥–ª—è train)
        preprocessor: –ì–æ—Ç–æ–≤—ã–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä (–¥–ª—è val/test)

    Returns:
        (processed_df, label_mapping, preprocessor)
    """
    if config is None:
        config = load_config()

    # –°–æ–∑–¥–∞—ë–º –∫–æ–ø–∏—é
    df = df.copy()

    if schema is None:
        schema = create_feature_schema(df, config)

    print("\n" + "="*60)
    print("–ü–†–ï–ü–†–û–¶–ï–°–°–ò–ù–ì")
    print("="*60)

    target_col = config["ingestion"]["target_column"]
    feature_cols = schema["feature_columns"]

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
    feature_cols = [c for c in feature_cols if c in df.columns]

    preprocessing_config = config["preprocessing"]
    labels_config = config["labels"]

    # 1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Å–æ–≤ (—É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –µ–¥–∏–Ω–æ–º—É –≤–∏–¥—É)
    print("\nüìä –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–∫...")
    df[target_col] = df[target_col].str.strip()

    # –£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π Web Attack (—Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω–∏—è)
    label_fixes = {
        'Web Attack Brute Force': 'Web Attack ‚Äì Brute Force',
        'Web Attack XSS': 'Web Attack ‚Äì XSS',
        'Web Attack Sql Injection': 'Web Attack ‚Äì Sql Injection',
        'Web Attack - Brute Force': 'Web Attack ‚Äì Brute Force',
        'Web Attack - XSS': 'Web Attack ‚Äì XSS',
        'Web Attack - Sql Injection': 'Web Attack ‚Äì Sql Injection',
    }
    df[target_col] = df[target_col].replace(label_fixes)

    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤: {df[target_col].nunique()}")

    # 2. –°–æ–∑–¥–∞–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω—ã—Ö –∏ –º—É–ª—å—Ç–∏–∫–ª–∞—Å—Å –º–µ—Ç–æ–∫
    print("\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–æ–∫...")

    # –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    binary_mapping = labels_config["binary_mapping"]
    df["label_binary"] = df[target_col].apply(
        lambda x: binary_mapping.get(x, binary_mapping["default"])
    )

    # –ú—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    multiclass_mapping = labels_config["multiclass_mapping"]
    max_class = max(multiclass_mapping.values())

    def get_multiclass_label(x):
        if x in multiclass_mapping:
            return multiclass_mapping[x]
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        for key, val in multiclass_mapping.items():
            if key.lower() in x.lower() or x.lower() in key.lower():
                return val
        return max_class + 1  # unknown

    df["label_multiclass"] = df[target_col].apply(get_multiclass_label)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º unknown –∫–ª–∞—Å—Å—ã
    unknown_count = (df["label_multiclass"] == max_class + 1).sum()
    if unknown_count > 0:
        unknown_labels = df[df["label_multiclass"] == max_class + 1][target_col].unique()
        print(f"   ‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤: {unknown_count} ({unknown_labels[:5]})")

    label_mapping = {
        "binary": binary_mapping,
        "multiclass": multiclass_mapping,
        "binary_column": "label_binary",
        "multiclass_column": "label_multiclass"
    }

    binary_dist = df['label_binary'].value_counts()
    print(f"   Binary - Benign: {binary_dist.get(0, 0):,}, Attack: {binary_dist.get(1, 0):,}")
    print(f"   Multiclass: {df['label_multiclass'].nunique()} unique classes")

    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf –ø–µ—Ä–µ–¥ —Å–∫–µ–π–ª–∏–Ω–≥–æ–º
    print(f"\nüìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ —Å–∫–µ–π–ª–∏–Ω–≥–æ–º...")

    # –ó–∞–º–µ–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    for col in feature_cols:
        # –ó–∞–º–µ–Ω—è–µ–º inf –Ω–∞ NaN, –∑–∞—Ç–µ–º NaN –Ω–∞ 0
        df.loc[:, col] = df[col].replace([np.inf, -np.inf], np.nan)
        if df[col].isna().sum() > 0:
            df.loc[:, col] = df[col].fillna(0)

    nan_count = df[feature_cols].isna().sum().sum()
    inf_count = sum(((df[col] == np.inf) | (df[col] == -np.inf)).sum() for col in feature_cols)
    print(f"   NaN: {nan_count}, Inf: {inf_count}")

    # 4. –°–∫–µ–π–ª–∏–Ω–≥
    print(f"\nüìä –°–∫–µ–π–ª–∏–Ω–≥ (–º–µ—Ç–æ–¥: {preprocessing_config['scaler']})...")

    scaler_type = preprocessing_config["scaler"]

    if fit:
        if scaler_type == "standard":
            scaler = StandardScaler()
        elif scaler_type == "robust":
            scaler = RobustScaler()
        elif scaler_type == "minmax":
            scaler = MinMaxScaler()
        else:
            raise ValueError(f"Unknown scaler: {scaler_type}")

        # –û–±—É—á–∞–µ–º —Å–∫–µ–π–ª–µ—Ä
        scaled_values = scaler.fit_transform(df[feature_cols])
        df.loc[:, feature_cols] = scaled_values
        preprocessor = {"scaler": scaler, "feature_cols": feature_cols}
    else:
        if preprocessor is None:
            raise ValueError("Preprocessor required when fit=False")
        scaler = preprocessor["scaler"]
        scaled_values = scaler.transform(df[feature_cols])
        df.loc[:, feature_cols] = scaled_values

    print(f"   Scaled {len(feature_cols)} features")

    # 5. –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
    print(f"\nüìä –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞...")
    print(f"   –°—Ç—Ä–æ–∫: {len(df):,}")
    print(f"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}")
    print(f"   –ö–æ–ª–æ–Ω–æ–∫ –≤—Å–µ–≥–æ: {len(df.columns)}")

    return df, label_mapping, preprocessor


def save_processed_data(
    df: pd.DataFrame,
    schema: Dict[str, Any],
    label_mapping: Dict[str, Any],
    preprocessor: Any,
    config: Optional[Dict[str, Any]] = None,
    output_path: Optional[Path] = None
) -> Dict[str, Path]:
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã"""
    if config is None:
        config = load_config()

    root = get_project_root()
    if output_path is None:
        output_path = root / config["paths"]["processed_data"]

    ensure_dir(output_path)
    artifacts_path = root / config["paths"]["artifacts"]
    ensure_dir(artifacts_path)

    saved_files = {}

    # 1. –î–∞–Ω–Ω—ã–µ
    data_path = output_path / "processed_data.parquet"
    df.to_parquet(data_path, index=False)
    saved_files["data"] = data_path
    print(f"\nüíæ Data saved to: {data_path}")
    print(f"   Size: {data_path.stat().st_size / (1024*1024):.1f} MB")

    # 2. –°—Ö–µ–º–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    schema_path = artifacts_path / "feature_schema.json"
    with open(schema_path, 'w', encoding='utf-8') as f:
        json.dump(schema, f, indent=2, ensure_ascii=False)
    saved_files["schema"] = schema_path
    print(f"üíæ Schema saved to: {schema_path}")

    # 3. –ú–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–æ–∫
    labels_path = artifacts_path / "label_mapping.json"
    with open(labels_path, 'w', encoding='utf-8') as f:
        json.dump(label_mapping, f, indent=2, ensure_ascii=False)
    saved_files["labels"] = labels_path
    print(f"üíæ Labels saved to: {labels_path}")

    # 4. –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    if config["preprocessing"]["save_preprocessor"]:
        preprocessor_path = artifacts_path / "preprocessor.joblib"
        joblib.dump(preprocessor, preprocessor_path)
        saved_files["preprocessor"] = preprocessor_path
        print(f"üíæ Preprocessor saved to: {preprocessor_path}")

    return saved_files