{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "cells": [
  {
   "id": "e974eaf2",
   "cell_type": "markdown",
   "source": "# CIC-IDS-2017: Model Training\n\nThis notebook is intended to run in Google Colab.\n\n**What this notebook does**\n1. Loads the prepared data splits\n2. Trains multiple models with different hyperparameters\n3. Compares validation and test results\n4. Builds an ensemble from the best models\n5. Saves the best models and artifacts\n",
   "metadata": {}
  },
  {
   "id": "55f6c7d8",
   "cell_type": "markdown",
   "source": "## 1. Install dependencies",
   "metadata": {}
  },
  {
   "id": "7766a4a7",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "!pip install -q xgboost lightgbm scikit-learn pandas numpy plotly kaleido joblib tqdm pyyaml",
   "outputs": []
  },
  {
   "id": "1f7fcee4",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "import os\nimport json\nimport time\nimport warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport joblib\nfrom tqdm.auto import tqdm\n\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, average_precision_score, confusion_matrix,\n    classification_report\n)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nwarnings.filterwarnings(\"ignore\")\nprint(\"Libraries imported.\")",
   "outputs": []
  },
  {
   "id": "c528cbcf",
   "cell_type": "markdown",
   "source": "## 2. Load data\n\nEnsure the following files exist in your runtime (or in the specified Google Drive paths):\n\n- `train.parquet`\n- `val.parquet`\n- `test.parquet`\n- `feature_schema.json`\n- `label_mapping.json`\n",
   "metadata": {}
  },
  {
   "id": "38a7357c",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "from google.colab import drive\ndrive.mount(\"/content/drive\")",
   "outputs": []
  },
  {
   "id": "f306b287",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "DATA_PATH = \"/content/drive/MyDrive/TraficAnalysis/data/processed/splits/\"\nARTIFACTS_PATH = \"/content/drive/MyDrive/TraficAnalysis/artifacts/\"\nOUTPUT_PATH = \"/content/drive/MyDrive/TraficAnalysis/models/\"\n\nos.makedirs(OUTPUT_PATH, exist_ok=True)",
   "outputs": []
  },
  {
   "id": "09d7f980",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "print(\"Loading data...\")\n\ntrain_df = pd.read_parquet(f\"{DATA_PATH}/train.parquet\")\nval_df = pd.read_parquet(f\"{DATA_PATH}/val.parquet\")\ntest_df = pd.read_parquet(f\"{DATA_PATH}/test.parquet\")\n\nprint(f\"Train: {len(train_df):,} rows\")\nprint(f\"Val:   {len(val_df):,} rows\")\nprint(f\"Test:  {len(test_df):,} rows\")\n\nwith open(f\"{ARTIFACTS_PATH}/feature_schema.json\", \"r\") as f:\n    feature_schema = json.load(f)\n\nwith open(f\"{ARTIFACTS_PATH}/label_mapping.json\", \"r\") as f:\n    label_mapping = json.load(f)\n\nfeature_cols = feature_schema[\"feature_columns\"]\nprint(f\"Features: {len(feature_cols)}\")",
   "outputs": []
  },
  {
   "id": "270779da",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "X_train = train_df[feature_cols].values\ny_train = train_df[\"label_binary\"].values\n\nX_val = val_df[feature_cols].values\ny_val = val_df[\"label_binary\"].values\n\nX_test = test_df[feature_cols].values\ny_test = test_df[\"label_binary\"].values\n\nprint(\"Data shapes:\")\nprint(f\"X_train: {X_train.shape}\")\nprint(f\"X_val:   {X_val.shape}\")\nprint(f\"X_test:  {X_test.shape}\")\n\nprint(\"Class balance (0=Benign, 1=Attack):\")\nprint(f\"Train - Benign: {(y_train==0).sum():,}, Attack: {(y_train==1).sum():,}\")\nprint(f\"Val   - Benign: {(y_val==0).sum():,}, Attack: {(y_val==1).sum():,}\")\nprint(f\"Test  - Benign: {(y_test==0).sum():,}, Attack: {(y_test==1).sum():,}\")",
   "outputs": []
  },
  {
   "id": "4bf73633",
   "cell_type": "markdown",
   "source": "## 3. Helper functions",
   "metadata": {}
  },
  {
   "id": "864f540d",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "def calculate_metrics(y_true, y_pred, y_proba=None):\n    metrics = {\n        \"accuracy\": accuracy_score(y_true, y_pred),\n        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n    }\n\n    if y_proba is not None:\n        if len(y_proba.shape) > 1:\n            y_proba = y_proba[:, 1]\n        try:\n            metrics[\"roc_auc\"] = roc_auc_score(y_true, y_proba)\n            metrics[\"pr_auc\"] = average_precision_score(y_true, y_proba)\n        except Exception:\n            metrics[\"roc_auc\"] = 0.0\n            metrics[\"pr_auc\"] = 0.0\n\n    return metrics\n\n\ndef train_and_evaluate(model, model_name, X_train, y_train, X_val, y_val, X_test, y_test):\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"Training: {model_name}\")\n    print(\"=\" * 60)\n\n    start_time = time.time()\n    model.fit(X_train, y_train)\n    training_time = time.time() - start_time\n\n    print(f\"Training time: {training_time:.1f}s\")\n\n    results = {\"name\": model_name, \"training_time\": training_time}\n\n    for split_name, X, y in [(\"train\", X_train, y_train), (\"val\", X_val, y_val), (\"test\", X_test, y_test)]:\n        y_pred = model.predict(X)\n        y_proba = model.predict_proba(X)\n\n        metrics = calculate_metrics(y, y_pred, y_proba)\n        for metric_name, value in metrics.items():\n            results[f\"{split_name}_{metric_name}\"] = value\n\n        if split_name == \"test\":\n            results[\"y_test_true\"] = y\n            results[\"y_test_pred\"] = y_pred\n            results[\"y_test_proba\"] = y_proba\n\n    print(f\"Val F1: {results['val_f1']:.4f}, Test F1: {results['test_f1']:.4f}\")\n    print(f\"Val ROC-AUC: {results['val_roc_auc']:.4f}, Test ROC-AUC: {results['test_roc_auc']:.4f}\")\n\n    return model, results\n\n\ndef print_results_table(all_results):\n    df = pd.DataFrame(all_results)\n\n    display_cols = [\"name\", \"training_time\", \"val_f1\", \"val_roc_auc\", \"test_f1\", \"test_roc_auc\"]\n    display_cols = [c for c in display_cols if c in df.columns]\n\n    df_display = df[display_cols].copy()\n    df_display = df_display.sort_values(\"val_f1\", ascending=False)\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"RESULTS SUMMARY\")\n    print(\"=\" * 80)\n    print(df_display.to_string(index=False))\n\n    return df_display",
   "outputs": []
  },
  {
   "id": "1ea90908",
   "cell_type": "markdown",
   "source": "## 4. Train models",
   "metadata": {}
  },
  {
   "id": "043789ec",
   "cell_type": "markdown",
   "source": "### 4.1 Random Forest",
   "metadata": {}
  },
  {
   "id": "55541b5a",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "rf_experiments = [\n    {\"name\": \"RF_baseline\", \"n_estimators\": 100, \"max_depth\": None},\n    {\"name\": \"RF_deep\", \"n_estimators\": 200, \"max_depth\": 20, \"min_samples_split\": 5},\n    {\"name\": \"RF_wide\", \"n_estimators\": 300, \"max_depth\": 10, \"min_samples_leaf\": 2},\n]\n\nrf_results = []\nrf_models = {}\n\nfor exp in tqdm(rf_experiments, desc=\"Random Forest experiments\"):\n    model = RandomForestClassifier(\n        n_estimators=exp.get(\"n_estimators\", 100),\n        max_depth=exp.get(\"max_depth\"),\n        min_samples_split=exp.get(\"min_samples_split\", 2),\n        min_samples_leaf=exp.get(\"min_samples_leaf\", 1),\n        class_weight=\"balanced\",\n        n_jobs=-1,\n        random_state=42,\n        verbose=0,\n    )\n\n    trained_model, results = train_and_evaluate(\n        model, exp[\"name\"], X_train, y_train, X_val, y_val, X_test, y_test\n    )\n\n    rf_results.append(results)\n    rf_models[exp[\"name\"]] = trained_model\n\nprint_results_table(rf_results)",
   "outputs": []
  },
  {
   "id": "a0360ec2",
   "cell_type": "markdown",
   "source": "### 4.2 XGBoost",
   "metadata": {}
  },
  {
   "id": "4385fa4c",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "xgb_experiments = [\n    {\"name\": \"XGB_baseline\", \"n_estimators\": 100, \"max_depth\": 6, \"learning_rate\": 0.1},\n    {\"name\": \"XGB_deep\", \"n_estimators\": 200, \"max_depth\": 10, \"learning_rate\": 0.05},\n    {\n        \"name\": \"XGB_regularized\",\n        \"n_estimators\": 150,\n        \"max_depth\": 8,\n        \"learning_rate\": 0.1,\n        \"reg_alpha\": 0.1,\n        \"reg_lambda\": 1.0,\n    },\n]\n\nscale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\nprint(f\"scale_pos_weight: {scale_pos_weight:.2f}\")\n\nxgb_results = []\nxgb_models = {}\n\nfor exp in tqdm(xgb_experiments, desc=\"XGBoost experiments\"):\n    model = xgb.XGBClassifier(\n        n_estimators=exp.get(\"n_estimators\", 100),\n        max_depth=exp.get(\"max_depth\", 6),\n        learning_rate=exp.get(\"learning_rate\", 0.1),\n        subsample=exp.get(\"subsample\", 0.8),\n        colsample_bytree=exp.get(\"colsample_bytree\", 0.8),\n        reg_alpha=exp.get(\"reg_alpha\", 0),\n        reg_lambda=exp.get(\"reg_lambda\", 1),\n        scale_pos_weight=scale_pos_weight,\n        use_label_encoder=False,\n        eval_metric=\"logloss\",\n        n_jobs=-1,\n        random_state=42,\n        verbosity=0,\n    )\n\n    trained_model, results = train_and_evaluate(\n        model, exp[\"name\"], X_train, y_train, X_val, y_val, X_test, y_test\n    )\n\n    xgb_results.append(results)\n    xgb_models[exp[\"name\"]] = trained_model\n\nprint_results_table(xgb_results)",
   "outputs": []
  },
  {
   "id": "1899c795",
   "cell_type": "markdown",
   "source": "### 4.3 LightGBM",
   "metadata": {}
  },
  {
   "id": "04be4a21",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "lgb_experiments = [\n    {\"name\": \"LGBM_baseline\", \"n_estimators\": 100, \"num_leaves\": 31, \"learning_rate\": 0.1},\n    {\"name\": \"LGBM_deep\", \"n_estimators\": 200, \"num_leaves\": 63, \"max_depth\": 10, \"learning_rate\": 0.05},\n    {\"name\": \"LGBM_fast\", \"n_estimators\": 300, \"num_leaves\": 15, \"learning_rate\": 0.15},\n]\n\nlgb_results = []\nlgb_models = {}\n\nfor exp in tqdm(lgb_experiments, desc=\"LightGBM experiments\"):\n    model = lgb.LGBMClassifier(\n        n_estimators=exp.get(\"n_estimators\", 100),\n        num_leaves=exp.get(\"num_leaves\", 31),\n        max_depth=exp.get(\"max_depth\", -1),\n        learning_rate=exp.get(\"learning_rate\", 0.1),\n        subsample=exp.get(\"subsample\", 0.8),\n        colsample_bytree=exp.get(\"colsample_bytree\", 0.8),\n        class_weight=\"balanced\",\n        n_jobs=-1,\n        random_state=42,\n        verbose=-1,\n    )\n\n    trained_model, results = train_and_evaluate(\n        model, exp[\"name\"], X_train, y_train, X_val, y_val, X_test, y_test\n    )\n\n    lgb_results.append(results)\n    lgb_models[exp[\"name\"]] = trained_model\n\nprint_results_table(lgb_results)",
   "outputs": []
  },
  {
   "id": "e9337cdf",
   "cell_type": "markdown",
   "source": "### 4.4 Neural Network",
   "metadata": {}
  },
  {
   "id": "45707932",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "nn_experiments = [\n    {\"name\": \"NN_small\", \"hidden_layer_sizes\": (64, 32), \"learning_rate_init\": 0.001},\n    {\"name\": \"NN_medium\", \"hidden_layer_sizes\": (128, 64, 32), \"learning_rate_init\": 0.001},\n    {\"name\": \"NN_large\", \"hidden_layer_sizes\": (256, 128, 64), \"learning_rate_init\": 0.0005},\n]\n\nnn_results = []\nnn_models = {}\n\nfor exp in tqdm(nn_experiments, desc=\"Neural Network experiments\"):\n    model = MLPClassifier(\n        hidden_layer_sizes=exp.get(\"hidden_layer_sizes\", (100,)),\n        activation=\"relu\",\n        learning_rate_init=exp.get(\"learning_rate_init\", 0.001),\n        max_iter=100,\n        early_stopping=True,\n        validation_fraction=0.1,\n        random_state=42,\n        verbose=False,\n    )\n\n    trained_model, results = train_and_evaluate(\n        model, exp[\"name\"], X_train, y_train, X_val, y_val, X_test, y_test\n    )\n\n    nn_results.append(results)\n    nn_models[exp[\"name\"]] = trained_model\n\nprint_results_table(nn_results)",
   "outputs": []
  },
  {
   "id": "b049bbbe",
   "cell_type": "markdown",
   "source": "## 5. Compare all models",
   "metadata": {}
  },
  {
   "id": "af76cb7c",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "all_results = rf_results + xgb_results + lgb_results + nn_results\nall_models = {**rf_models, **xgb_models, **lgb_models, **nn_models}\n\nresults_df = pd.DataFrame(all_results).sort_values(\"val_f1\", ascending=False)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ALL MODELS COMPARISON (sorted by Val F1)\")\nprint(\"=\" * 80)\n\ndisplay_cols = [\n    \"name\",\n    \"training_time\",\n    \"val_f1\",\n    \"val_roc_auc\",\n    \"val_pr_auc\",\n    \"test_f1\",\n    \"test_roc_auc\",\n    \"test_pr_auc\",\n]\nprint(results_df[display_cols].to_string(index=False))",
   "outputs": []
  },
  {
   "id": "3d919a1c",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "fig = go.Figure()\n\nmetrics_to_plot = [\"f1\", \"roc_auc\", \"precision\", \"recall\"]\ncolors = px.colors.qualitative.Set2\n\nfor i, metric in enumerate(metrics_to_plot):\n    fig.add_trace(\n        go.Bar(\n            name=metric.upper(),\n            x=results_df[\"name\"],\n            y=results_df[f\"test_{metric}\"],\n            marker_color=colors[i],\n        )\n    )\n\nfig.update_layout(\n    title=\"Model Comparison (Test Set)\",\n    xaxis_title=\"Model\",\n    yaxis_title=\"Score\",\n    barmode=\"group\",\n    height=500,\n    width=1000,\n)\n\nfig.show()",
   "outputs": []
  },
  {
   "id": "d4711b98",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "from sklearn.metrics import roc_curve\n\nfig = go.Figure()\n\nfor name, model in all_models.items():\n    y_proba = model.predict_proba(X_test)\n    if len(y_proba.shape) > 1:\n        y_proba = y_proba[:, 1]\n\n    fpr, tpr, _ = roc_curve(y_test, y_proba)\n    auc = roc_auc_score(y_test, y_proba)\n\n    fig.add_trace(\n        go.Scatter(\n            x=fpr,\n            y=tpr,\n            mode=\"lines\",\n            name=f\"{name} (AUC={auc:.3f})\",\n        )\n    )\n\nfig.add_trace(\n    go.Scatter(\n        x=[0, 1],\n        y=[0, 1],\n        mode=\"lines\",\n        name=\"Random\",\n        line=dict(dash=\"dash\", color=\"gray\"),\n    )\n)\n\nfig.update_layout(\n    title=\"ROC Curves - All Models\",\n    xaxis_title=\"False Positive Rate\",\n    yaxis_title=\"True Positive Rate\",\n    height=600,\n    width=800,\n)\n\nfig.show()",
   "outputs": []
  },
  {
   "id": "6e79ef0a",
   "cell_type": "markdown",
   "source": "## 6. Build an ensemble",
   "metadata": {}
  },
  {
   "id": "53997660",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "top_k = 5\ntop_models_df = results_df.head(top_k)\nprint(f\"Top-{top_k} models for the ensemble:\")\nprint(top_models_df[[\"name\", \"val_f1\", \"test_f1\"]].to_string(index=False))\n\ntop_model_names = top_models_df[\"name\"].tolist()\ntop_models = {name: all_models[name] for name in top_model_names}",
   "outputs": []
  },
  {
   "id": "66f0c297",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "class SimpleEnsemble:\n    def __init__(self, models, weights=None):\n        self.models = models\n        if weights is None:\n            self.weights = [1.0 / len(models)] * len(models)\n        else:\n            total = sum(weights)\n            self.weights = [w / total for w in weights]\n\n    def predict_proba(self, X):\n        probas = []\n        for model in self.models.values():\n            probas.append(model.predict_proba(X))\n\n        weighted_proba = np.zeros_like(probas[0])\n        for proba, weight in zip(probas, self.weights):\n            weighted_proba += proba * weight\n\n        return weighted_proba\n\n    def predict(self, X):\n        proba = self.predict_proba(X)\n        return np.argmax(proba, axis=1)\n\n\nensemble_equal = SimpleEnsemble(top_models)\n\nweights = top_models_df[\"val_f1\"].tolist()\nensemble_weighted = SimpleEnsemble(top_models, weights=weights)",
   "outputs": []
  },
  {
   "id": "ae01f77c",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "print(\"\\n\" + \"=\" * 60)\nprint(\"ENSEMBLE RESULTS\")\nprint(\"=\" * 60)\n\nfor name, ensemble in [(\"Ensemble_Equal\", ensemble_equal), (\"Ensemble_Weighted\", ensemble_weighted)]:\n    y_pred = ensemble.predict(X_test)\n    y_proba = ensemble.predict_proba(X_test)\n\n    metrics = calculate_metrics(y_test, y_pred, y_proba)\n\n    print(f\"\\n{name}:\")\n    print(f\"F1:        {metrics['f1']:.4f}\")\n    print(f\"ROC-AUC:   {metrics['roc_auc']:.4f}\")\n    print(f\"PR-AUC:    {metrics['pr_auc']:.4f}\")\n    print(f\"Precision: {metrics['precision']:.4f}\")\n    print(f\"Recall:    {metrics['recall']:.4f}\")",
   "outputs": []
  },
  {
   "id": "827e2e69",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "best_single = results_df.iloc[0]\n\ny_pred_best = all_models[best_single[\"name\"]].predict(X_test)\ny_proba_best = all_models[best_single[\"name\"]].predict_proba(X_test)\nmetrics_best = calculate_metrics(y_test, y_pred_best, y_proba_best)\n\ny_pred_ens = ensemble_weighted.predict(X_test)\ny_proba_ens = ensemble_weighted.predict_proba(X_test)\nmetrics_ens = calculate_metrics(y_test, y_pred_ens, y_proba_ens)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"BEST SINGLE MODEL vs ENSEMBLE\")\nprint(\"=\" * 60)\n\nprint(f\"\\n{'Metric':<15} {'Best Single':<15} {'Ensemble':<15} {'Diff':<10}\")\nprint(\"-\" * 55)\nfor metric in [\"f1\", \"roc_auc\", \"pr_auc\", \"precision\", \"recall\"]:\n    diff = metrics_ens[metric] - metrics_best[metric]\n    sign = \"+\" if diff > 0 else \"\"\n    print(f\"{metric:<15} {metrics_best[metric]:<15.4f} {metrics_ens[metric]:<15.4f} {sign}{diff:.4f}\")",
   "outputs": []
  },
  {
   "id": "092aed71",
   "cell_type": "markdown",
   "source": "## 7. Save models",
   "metadata": {}
  },
  {
   "id": "87861be9",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "best_model_name = results_df.iloc[0][\"name\"]\nbest_model = all_models[best_model_name]\n\nbest_model_path = f\"{OUTPUT_PATH}/best_model_{best_model_name}.joblib\"\njoblib.dump(best_model, best_model_path)\nprint(f\"Best model saved: {best_model_path}\")\n\nfor name, model in top_models.items():\n    model_path = f\"{OUTPUT_PATH}/{name}.joblib\"\n    joblib.dump(model, model_path)\n    print(f\"Model saved: {model_path}\")\n\nresults_df.to_csv(f\"{OUTPUT_PATH}/experiment_results.csv\", index=False)\nprint(f\"Results saved: {OUTPUT_PATH}/experiment_results.csv\")\n\nensemble_config = {\n    \"models\": top_model_names,\n    \"weights\": weights,\n    \"voting\": \"soft\",\n    \"val_f1\": float(results_df.iloc[0][\"val_f1\"]),\n    \"test_f1\": float(metrics_ens[\"f1\"]),\n    \"test_roc_auc\": float(metrics_ens[\"roc_auc\"]),\n}\n\nwith open(f\"{OUTPUT_PATH}/ensemble_config.json\", \"w\") as f:\n    json.dump(ensemble_config, f, indent=2)\nprint(f\"Ensemble config saved: {OUTPUT_PATH}/ensemble_config.json\")",
   "outputs": []
  },
  {
   "id": "92fe54c0",
   "cell_type": "markdown",
   "source": "## 8. Final report",
   "metadata": {}
  },
  {
   "id": "28944152",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(y_test, y_pred_ens)\ncm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nsns.heatmap(\n    cm,\n    annot=True,\n    fmt=\"d\",\n    cmap=\"Blues\",\n    ax=axes[0],\n    xticklabels=[\"Benign\", \"Attack\"],\n    yticklabels=[\"Benign\", \"Attack\"],\n)\naxes[0].set_title(\"Confusion Matrix (Counts)\")\naxes[0].set_xlabel(\"Predicted\")\naxes[0].set_ylabel(\"Actual\")\n\nsns.heatmap(\n    cm_normalized,\n    annot=True,\n    fmt=\".2%\",\n    cmap=\"Blues\",\n    ax=axes[1],\n    xticklabels=[\"Benign\", \"Attack\"],\n    yticklabels=[\"Benign\", \"Attack\"],\n)\naxes[1].set_title(\"Confusion Matrix (Normalized)\")\naxes[1].set_xlabel(\"Predicted\")\naxes[1].set_ylabel(\"Actual\")\n\nplt.tight_layout()\nplt.savefig(f\"{OUTPUT_PATH}/confusion_matrix.png\", dpi=150)\nplt.show()",
   "outputs": []
  },
  {
   "id": "45e844ee",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "print(\"\\n\" + \"=\" * 60)\nprint(\"CLASSIFICATION REPORT (Ensemble)\")\nprint(\"=\" * 60)\nprint(classification_report(y_test, y_pred_ens, target_names=[\"Benign\", \"Attack\"]))",
   "outputs": []
  },
  {
   "id": "61e4db9f",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "if \"XGB_baseline\" in all_models:\n    xgb_model = all_models[\"XGB_baseline\"]\n    importance = xgb_model.feature_importances_\n\n    importance_df = pd.DataFrame({\"feature\": feature_cols, \"importance\": importance}).sort_values(\n        \"importance\", ascending=False\n    )\n\n    top_20 = importance_df.head(20)\n\n    fig = go.Figure(\n        go.Bar(\n            x=top_20[\"importance\"][::-1],\n            y=top_20[\"feature\"][::-1],\n            orientation=\"h\",\n            marker_color=\"steelblue\",\n        )\n    )\n\n    fig.update_layout(\n        title=\"Top-20 Feature Importance (XGBoost)\",\n        xaxis_title=\"Importance\",\n        yaxis_title=\"Feature\",\n        height=600,\n        width=800,\n        margin=dict(l=200),\n    )\n\n    fig.show()\n\n    importance_df.to_csv(f\"{OUTPUT_PATH}/feature_importance.csv\", index=False)",
   "outputs": []
  },
  {
   "id": "10a72876",
   "cell_type": "markdown",
   "source": "## Done\n\n**Artifacts in `models/`**\n- `best_model_*.joblib`: best single model (by validation F1)\n- `*.joblib`: top-5 models used for the ensemble\n- `experiment_results.csv`: results for all experiments\n- `ensemble_config.json`: ensemble configuration and summary metrics\n- `confusion_matrix.png`: confusion matrix (ensemble)\n- `feature_importance.csv`: XGBoost feature importances (if available)\n",
   "metadata": {}
  }
 ]
}